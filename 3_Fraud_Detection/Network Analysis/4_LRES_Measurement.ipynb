{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c67a4b2",
   "metadata": {},
   "source": [
    "## SOCIAL NETWORK ANALYSIS (PART II)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da694245",
   "metadata": {},
   "source": [
    "\n",
    "When exploring the question of how social networks affect the spread of ideas, innovation, and behaviour, the first key aspect to consider is a network's structure or topology. Who a person is connected to is who they come into contact with, and therefore who they \"allow\" to have an impact on their behaviour. The extent to which this affects the individual is predicated on how they \"choose\" to come into contact with this person: how close they are, how frequent the contact is, is it through a close knit group of mutual friends or through separate interactions? Given the many factors that determine the diffusion (spread of information) process, a common first step approach is to examine the network for \"small world\" features. Small worlds characterise a fascinating aspect of large social networks that find that even if nodes in a network are not directly connected, the likelihood that their neighbours are each others neighbours is high. This means any given node in a network can be reached in a small number of steps, which translates to a short average path length and a high clustering coefficient. Naturally, this creates an easy path for diffusion of information, where higher levels of cooperation and  efficiency are more likely.  \n",
    "\n",
    "Though exhibiting small world properties, most real world networks display a scale-free degree distribution which basically means the more popular a node is, the more it attracts other nodes. Moreover, when faced with a group of people, each person could have a different degree of connectivity and therefore vary in their threshold or vulnerability to social influence and peer pressure. The implication of this is that when determining a networks most efficient path of diffusion and information flow, we need to consider a node's connectivity and also the connectivity of a nodes closest connections or \"neighbourhood\". In other words, while one person can adopt a behaviour after coming in contact with a single \"infected\" (already adopted) person, another person requires multiple sources of contact with a group of “infected” people before they can be convinced of taking up the behavior. In heavly clustered or dense networks, the presence of redundant (path independent) ties can act as a social reinforcement mechanism (or multiplier effect). First, by quickly circulating information within smaller groups, then, by bridging information across wider-reaching groups. In contrast, when the connections a person makes become heavily reliant on the shared interests, backgrounds, and demographics of the people they are connecting with, then this tendency toward grouping, described as homophily, has much greater implication for the social divisions and social fragmentations caused. Those that do not conform or those that do not exhit certain characteristics are potentially excluded from social and economic benefits that come with having access to information and opportunities. Thus, depending on the degree of systematic exclusion displayed in the network, the presence of homophilic ties could lead to and exacerbate problems of inequality, immobility, and productivity inefficiency.\n",
    "\n",
    "The role that graph partitioning methods play in diffusion problems is very important as they provide us with a way to detect sub-communities and groups within the network. In other words, it gives us a practical way to test how the difussion process occurs in a network, and whether all members of the network are able to reach a uniform consensus, and how long it would take given the person that initially introduced the idea or information. For example, when new information on a claim is shared within a community, will only a small subset of people benefit from this information and does this outcome change depending on where or wwho the in information is coming from. These type of simulations essentially try to describe a scenario where more influential people and more people with connections\n",
    "\n",
    "\n",
    " Uncovering cohesive subgroups and communities in a network can especially help shape the type of interventions introduced in environments where the objective is to encourage participation and cooperation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Definition\n",
    "A group or community is cohesive to the extent that it is robust to disruption. In network science, this is captured by node connectivity.  A few other definitions outlined by Moody and White (2003):  \n",
    "\n",
    "- Definition 1: A collectivity is structurally cohesive to the extent that the social relations of its members hold it together.\n",
    "\n",
    "- Definition 2: A group is structurally cohesive to the extent that multiple independent relational paths among all pairs of members hold it together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea4ad6",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "\n",
    "Networks of people are often made up of subsets that interact more intensely among each other than they do with the rest of the network, and it is often very important in research and analysis to identify or approximate these subsets as best as possible so that they can be studied more closely. \n",
    "\n",
    "To achieve this, we start by applying vertex partitioning techniques that allow us to identify the networks core as well as tightly connected cliques. Next we find the optimal partition point for detecting densely connected communities or subgroups. Lastly, we which, though not disconnected from other parts of the graph, have higher levels of density between each other than with the rest of the network.\n",
    "\n",
    "To start, the **vertex partitioning**, **community detection**, and **spectral clustering** methods are described, then an evaluation of **restitution participation** properties is presented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09932ae5-f6af-4ebf-9d40-0c6ff8fa1d55",
   "metadata": {},
   "source": [
    "#### 1. VERTEX PARTITIONING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8689f5-5bfb-4150-a357-faece86660cd",
   "metadata": {},
   "source": [
    "Often graphs will be connected, but we still want to divide the vertices up into mutually exclusive subgroups of interest. Such a division is called a partition of a graph. In a partition, all vertices must be in one and only one subgroup. Partitions are created through making cuts in a graph.\n",
    "\n",
    ">***What Is A Cut-Set and Cut-Point***\n",
    ">\n",
    ">Cut-sets describe the connectivity of the graph based on the removal of nodes or vertices, while cut-points describe the connectivity of the graph based on the removal of edges.\n",
    ">*k* refers to the number of nodes or lines that would need to be removed to reduce the graph to a disconnected state.\n",
    ">For each connectivity value (*k*) observed in a given network, there is a unique set of subgroups with this level of structural cohesion.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5237e66-c2c2-4f7d-a64f-2092e756cb34",
   "metadata": {},
   "source": [
    "##### 1.1. COMPONENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc56a49-2318-4a45-9ee9-ecca984f3ddc",
   "metadata": {},
   "source": [
    ">***What Is A Component***\n",
    ">\n",
    ">The most basic form of network group is a component. In a connected component, every node is reachable via some path by every other node. Most network datasets have only a single large connected component with a few isolates - however, some unique datasets might have three or four large, distinct components.\n",
    ">\n",
    ">In a directed graph, components can be weakly or strongly connected. If node i can reach j via a directed path and j can reach i via a directed path, for all i and j nodes in the component, then we say the component is strongly connected. If all nodes are only reachable from a single direction, (i.e. i can reach j via a directed path, but j can’t reach i), then we say the component is weakly connected.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78009e8-0bd3-41bf-bb4f-145a5686f1db",
   "metadata": {},
   "source": [
    "##### 1.2. K-CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07dbc1f-d63d-400e-8d98-033552abdb31",
   "metadata": {},
   "source": [
    ">***What Is A K-Core***\n",
    ">\n",
    "> The k-core of a graph is a maximal subgraph in which each vertex has at least degree k (degree here means the degree in the subgraph). The coreness of a vertex is k if it is a member of the k-core but not a member of the k + 1-core. \n",
    ">The k-core is found by recursively pruning nodes with degrees less than k.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b949a85-ec7f-44f6-a68e-746fe807c0c4",
   "metadata": {},
   "source": [
    "##### 1.3. CLIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7aae66-1f5d-437f-8dd7-754adc47d236",
   "metadata": {},
   "source": [
    ">***What Is A Clique***\n",
    ">\n",
    ">A clique is a subset of vertices in an undirected graph whose induced subgraph is complete. That is, the induced subgraph has an edge density of 1. This is best understood as the most intense possible type of community in an undirected graph. \n",
    ">A maximal clique is a clique which cannot be extended by adding another vertex. In other words, it consists of a subset of nodes, all of which are adjacent to each other, and where there are no other nodes that are also adjacent to all of the members of the clique.\n",
    "> A largest clique is a clique with the greatest number of vertices of all cliques in the graph.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6191e-9696-46b6-8e91-45342390476d",
   "metadata": {},
   "source": [
    "##### 1.4. COHESIVE BLOCKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd954d-40f9-4b94-ab8d-dfe4e5644e37",
   "metadata": {},
   "source": [
    ">***What Is Cohesive Blocking***\n",
    ">\n",
    ">Cohesive blocking builds on the idea of cliques. It starts at the level of the component and identifies large substructures nested within the component. It then moves to those large substructures and identifies smaller and smaller nested substructures, until it reaches cliques. It is therefore a useful way to operationalize network embeddedness.\n",
    ">\n",
    ">Identification involves a recursive process: One first identifies the k-connectivity of an input graph, then removes the k-cutset(s) that hold(s) the network together. One then repeats this procedure on the resulting subgraphs, until no further cutting can be done. As such, any (k+l)-connected set embedded within the network will be identified. Moreover, each iteration of the procedure takes us deeper into the network, as weakly connected nodes are removed first, leaving stronger and stronger connected sets, uncovering the nested structure of cohesion in a network.\n",
    "\n",
    ">Because this method provides the ability to both identify cohesive groups and identify the position of each group in the overall structure, the method is called cohesive blocking. It is important to note the flexibility of this approach. The concept of cohesion presented here provides a way of ordering groups within hierarchically nested trees, with traditional segmented groups occupying separate branches of the cohesion structure, but allowing overlap between groups in different branches. The ability to accommodate both nested and segmented structures within a common frame is a strength of our model. Furthermore, considering that the process for identifying the nested connectivity sets is based on identifying the most fragile points in a network, those actors who are involved in the most highly connected portions of the network are often deeply insulated from perturbations in the overall network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322c958-b3d1-4a31-9bf6-6de9f74e0c49",
   "metadata": {},
   "source": [
    "#### 2. COMMUNITY DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d05bf",
   "metadata": {},
   "source": [
    "##### 2.1. OPTIMISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ea9ec",
   "metadata": {},
   "source": [
    "\n",
    "Finding the optimal solution to detection problems can be somewhat challenging, especially since there can be any number of communities in a given network and each of varying sizes. One method that tries to uncover optimal communities is **modularity optimization**. \n",
    "\n",
    ">***What Is Modularity***\n",
    ">\n",
    ">Modularity is a measure of the number of intra-community ties or within-group ties (relative to the expected number of ties formed when running a random process). The modularity algorithm is optimized such that the nodes inside a community are densely connected within community and sparsely connected between communities. Modularity scores of +1 mean that all the edges in a community are connecting nodes within the community. A score of 0 would mean that the community has half its edges connecting nodes within the same community, and half connecting nodes outside the community. A score of -1 means that there are no edges connecting nodes within the community, and they instead all connect nodes outside the community. A modularity score of 0.7 and above is a generally accepted sign of a good partition. \n",
    "\n",
    "Maximizing modularity is computationally complex, thus various heuristic approaches are applied when trying to locate good \"local\" maxima of modularity. Even if these heuristics cannot make any guarantees about the true global optimum, it is still good practice to try let the data “tell” you where “good” or “robust” values of gamma or the resolution parameter might be. A good heauristic for optimising the resolution parameter (gamma) is by plotting the number of communities at different values of gamma.\n",
    "\n",
    ">**What Is A Resolution Parameter (Gamma)**\n",
    ">\n",
    ">Resolution is a parameter that affects the size of recovered clusters or controls the number of communities detected. Higher resolutions lead to more communities of smaller sized clusters, while lower resolutions lead to fewer communities of larger sized clusters.\n",
    "\n",
    "When choosing the gamma, it is best to find the point that results in robust and resilient community detection. This is at the point on the graph where the number of communities detected seemingly plateaus (i.e. the number of communities detected is constant for atleast 2 or more consecutive gamma values). It is also good to note that the smaller the gamma value, the less penalty for weakly connected nodes to be grouped together in the same community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179fc33",
   "metadata": {},
   "source": [
    "##### 2.2. DETECTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d4aa7",
   "metadata": {},
   "source": [
    " \n",
    "Three popular modularity-based techniques for uncovering community structure are the **Greedy**, **Louvain** and **Leiden** algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae681488-c253-4e78-bad0-3d4636a2bf0a",
   "metadata": {},
   "source": [
    "**Fast Greedy Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330adf49-dbb0-4c75-85f5-24a6311705f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ">***What is Fast Greedy Method***\n",
    ">\n",
    ">The fast greedy method is an efficient approach to detect communities based on modularity. This strategy starts with a subnetwork composed only of links between highly connected nodes. Then, the algorithm iteratively samples random links that improve the modularity of the subnetwork and adds them. This iterative process is repeated as long as the modularity keeps improving. Finally, the communities are obtained based on the connected components in the subnetwork.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ede4ec",
   "metadata": {},
   "source": [
    "**Community Louvain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f19b76",
   "metadata": {},
   "source": [
    "\n",
    ">***What is Louvain Method***\n",
    ">\n",
    ">The louvain method is a community structure based on the multilevel algorithm of Blondel et al (2008). This is a bottom-up algorithm that starts with a weighted network of *N* nodes. In the first phase, the algorithm assigns a different community to each node of the network. Then for each node, it considers the neighbours and evaluates the gain of modularity by removing the particular node from the current community and placing it in the neighbour’s community. The node will be placed in the neighbour’s community if the gain is positive and maximized. The node will remain in the same community if there is no positive gain. This process is applied repeatedly and for all nodes until no further improvement is found. The first phase of the louvain algorithm stops when a local maxima of modularity is obtained. In the second phase, the algorithm builds a new network considering communities found in the first phase as nodes. Once the second phase is completed, the algorithm will reapply the first phase to the resulting network. These steps are repeated until there are no changes in the network and maximum modularity is obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139b4d9",
   "metadata": {},
   "source": [
    "**Community Leiden**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e3ee1",
   "metadata": {},
   "source": [
    "\n",
    ">***What is Leiden Method***\n",
    ">\n",
    ">The leiden method finds the community structure of the graph using the leiden algorithm of Traag, van Eck & Waltman (2019). It is an adaptation of the louvain method, designed to improve the tendency for the louvain community detection algorithm to discover communities that are internally disconnected (weakly connected communities). \n",
    ">In addition to the phases used in louvain algorithm, leiden uses one more phase which tries to refine the discovered partitions. In the refinement phase, the algorithm tries to identify refined partitions from the partitions proposed by the first phase. Communities proposed by the first phase may further split into multiple partitions in the second phase. The refinement phase does not follow a greedy approach and may merge a node with a randomly chosen community which increases the quality function. This randomness allows discovering the partition space more broadly. Also in the first phase, leiden follows a different approach to the louvain. Instead of visiting all the nodes in the network after the first visit to all nodes has been completed, leiden only visits those nodes whose neighbourhood has changed.\n",
    "\n",
    "In the analysis that follows, I use community leiden as my primary community detection method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee60373",
   "metadata": {},
   "source": [
    "##### 2.3. EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d0708",
   "metadata": {},
   "source": [
    " \n",
    "There are various quantitative measures for comparing two partitions against each other. Some of the most popular are pair counting scores like \"Rand\" and \"adjusted Rand\"; and the various information-theoretic measures like \"variation of information\", \"normalized mutual information\", and \"adjusted mutual information\". \n",
    "\n",
    ">***What is Variation of Information (VI)***\n",
    ">\n",
    ">The variation of information or shared information distance is a measure of the distance between two clusterings (partitions of elements). As a standardised score, the variation of information value ranges between 0 (no correlation) and 1 (perfect correlation).\n",
    "\n",
    ">***What is Adjusted Rand Index (ARI)***\n",
    ">\n",
    ">The adjusted rand index is introduced to determine whether two cluster results are similar to each other. It calculates the similarity between two cluster results by taking all points identified within the same cluster. In general, an ARI value lies between 0 and 1. The index value is equal to 1 only if a partition is completely identical to the intrinsic structure and close to 0 following a random partition.\n",
    "\n",
    ">***What is Normalized Mutual Information (NMI)***\n",
    ">\n",
    ">Normalized mutual information is a measure used to evaluate network partitioning performed by community finding algorithms. Normalized mutual information is a normalisation of the mutual information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation).\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189781fa-4174-4598-8599-72a09cf96fdd",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. SPECTRAL CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22acfec8-927d-4f6d-af85-5d88d0745f6a",
   "metadata": {},
   "source": [
    "\n",
    "Spectral clustering is a widely used unsupervised learning method which often outperforms other approaches..In spectral clustering, the affinity, and not the absolute location (i.e. k-means), determines what points fall under which cluster. Furthermore, the grouping is such that points in a cluster are similar to each other, and less similar to points in other clusters\n",
    "\n",
    "There are 2 broad approaches for clustering:\n",
    "\n",
    "1. *Compactness* — Points that lie close to each other fall in the same cluster and are compact around the cluster center. The closeness can be measured by the distance between the observations. E.g.: K-Means Clustering\n",
    "\n",
    "2. *Connectivity* — Points that are connected or immediately next to each other are put in the same cluster. Even if the distance between 2 points is less, if they are not connected, they are not clustered together. Spectral clustering is a technique that follows this approach.\n",
    "\n",
    "The spectral clustering algorithm is utilized to partition graphs in K groups based on their connectivity. The steps involved in spectral clustering include the below.\n",
    "\n",
    "1. Determine the Adjacency matrix W, Degree matrix D and the Laplacian matrix L\n",
    "2. Compute the eigenvectors of the matrix L\n",
    "3. Using the second smallest eigenvector as input, train a k-means model and use it to classify the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea74e7-e95b-4b2a-ba05-715d225d7919",
   "metadata": {},
   "source": [
    "##### 3.1. PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3ab0d-ea3d-4e6c-92d8-1792ef34a862",
   "metadata": {},
   "source": [
    "Constructing a Laplacian Matrix of the Graph. The Laplacian Matrix is a difference between Adjacency Matrix and the Diagonal Matrix. L = D — A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe5e08-09bc-4540-abb5-cccced53b2bf",
   "metadata": {},
   "source": [
    "##### 3.2. DECOMPOSITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840c566-0d57-48af-9244-5ba7aa13070a",
   "metadata": {},
   "source": [
    "Compute eigenvalues and eigenvectors of the Laplacian Matrix, following which we need to map each point to a lower-dimensional representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60be345-d2d6-4a41-b096-d1e2b010b652",
   "metadata": {},
   "source": [
    "##### 3.3. K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee672f-c165-4722-a45f-f2c7ab211543",
   "metadata": {},
   "source": [
    "Create groups of clusters by applying k-means algorithm to above data.\n",
    "\n",
    ">***What Is K-Means Algorithm***\n",
    ">\n",
    ">K-means algorithm is an iterative algorithm that divides a group of *n* datasets into K subgroups (clusters) based their mean distance from the centroid of that particular subgroup (cluster) formed. K, here is the pre-defined number of clusters to be formed by the Algorithm. If K=3, It means the number of clusters to be formed from the dataset is 3\n",
    "\n",
    "\n",
    "The working of the K-Means algorithm is explained in the below steps:\n",
    "\n",
    "1. Select the value of K, to decide the number of clusters to be formed.\n",
    "2. Select random K points which will act as centroids.\n",
    "3. Assign each data point, based on their distance from the randomly selected points (Centroid), to the nearest/closest centroid which will form the predefined clusters.\n",
    "4. place a new centroid of each cluster.\n",
    "5. Repeat step no.3, which reassign each datapoint to the new closest centroid of each cluster.\n",
    "6. If any reassignment occurs, then go to step-4 else go to Step 7.\n",
    "7. FINISH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7d4d8-0ed4-49b2-bbdc-6404f8d1a964",
   "metadata": {},
   "source": [
    "\n",
    "A pre-requirement of k-means clustering is that the number of clusters to be detected by the model be declared prior to running the model. In other words, you need to tell the model how many clusters to group the data in. The challenge, however, is in knowing the right number of clusters to specify. Fortunately, there are two popular techniques to assist in detecting the optimal number of clusters to fit the data. These are the *Elbow Method* and *Silhouette Method*.\n",
    "\n",
    ">***What is The Elbow Method***\n",
    ">\n",
    ">The elbow method is a commonly used graphical representation of finding an optimum number of clusters (K). This method uses within clusters sum of squares (WCSS) which accounts for the total variation within a cluster. Since this is a measure of error, the objective of k-means is to try to minimize this value. As the number of clusters increases, the variance decreases.\n",
    ">\n",
    ">The results of this method are shown by plotting the Elbow curve, where the x-axis represents the number of clusters and the y-axis an evaluation metric like inertia or in this case WCSS. The idea is to specify a range of clusters and evaluate at which point the decrease in inertia starts to slow or become constant. In other words, the point at which the curve starts to flatten.\n",
    "\n",
    "\n",
    ">***What is The Silhouette Method***\n",
    ">\n",
    "\n",
    ">The silhouette coefficient is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    ">1. How close the data point is to other points in the cluster.\n",
    ">2. How far away the data point is from points in other clusters.\n",
    ">\n",
    ">Silhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters. A score of 1-Points are perfectly assigned in a cluster and clusters are easily distinguishable, 0-Clusters are overlapping, and -1-Points are wrongly assigned in a cluster.\n",
    "\n",
    "> The following conditions should be checked to pick the right ‘K’ using the Silhouette plots:\n",
    ">1. For a particular K, all the clusters should have a Silhouette score more than the average score of the dataset (represented by a red dotted line). The x-axis represents the Silhouette score. \n",
    ">2. There should not be wide fluctuations in the size of the clusters. The width of the clusters represents the number of data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4620b-4e9c-4a7f-881f-f7408845233c",
   "metadata": {},
   "source": [
    "#### 4. RESTITUTION PARTICIPATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a8f170-e671-44c7-8e45-3aefcbb996a7",
   "metadata": {},
   "source": [
    "\n",
    "The final section looks at the communities participation in the land restitution programme. To determine if there is a systematic tie formation or grouping process at play, we visualise community members responses to questions on:\n",
    "\n",
    "- Attendance at the options workshop, \n",
    "- Involvement in a settlement committee, \n",
    "- Choice of cash compensation, \n",
    "- Exposure to rumours and misinformation. \n",
    "\n",
    "This provides us with a visual representation of a communities tendency toward homophilic ties, i.e. how often people form bonds with other similar people and therefore exhibit similar behavior. If true, then participation, voting, and diffusion of information can be explained by the degree of connectivity and therefore fragmentation in the community.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a46c7",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251b8c4",
   "metadata": {},
   "source": [
    "Goyal, Sanjeev., Connections: An Introduction to the Economics of Networks, Princeton, NJ: Princeton University Press, 2007.\n",
    "\n",
    "Jackson, Matthew O., Social and Economic Networks, Princeton, NJ: Princeton University Press, 2008.\n",
    "\n",
    "Wasserman, Stanley and Faust, Katherine, Social Network Analysis, New York, NY: Cambridge University Press, 2007.\n",
    "\n",
    "Watts, Duncan J., Small Worlds: the dynamics of networks between order and randomness, Princeton, NJ: Princeton University Press, 2005.\n",
    "\n",
    "Blondel, V., Guillaume, J., Lambiotte, R. and Lefebvre, E., 2008. Fast unfolding of communities in large networks. IOPscience.\n",
    "\n",
    "Newman, M. E. J., “Modularity and community structure in networks,” PNAS, 2006, 103 (23), 8577–8582.\n",
    "\n",
    "Newman and M. Girvan, “Finding and evaluating community structure in networks,” Physical Review E, 2004, 69 (2).\n",
    "\n",
    "Newman, M. E. J., \"Fast algorithm for detecting community structure in networks,\" Physical review E, 2004,  69 (6).\n",
    "\n",
    "Traag, V. A., Waltman, L. and van Eck N. J., “From Louvain to Leiden: guaranteeing well-connected communities,” Scientific Reports, 2019, 9 (5233).\n",
    "\n",
    "Moody, J. and White, D. R., \"Structural Cohesion and Embeddedness: A Hierarchical Concept of Social Groups,\" American Sociological Review, 2003, 68 (1), 103-127.\n",
    "\n",
    "Mucha, Peter., \"Community Detection,\" Social Networks and Health Workshop, 2019. Available at: https://sites.duke.edu/dnac/13-community-detection/\n",
    "\n",
    "Moody, J., \"Network Visualisation and Communities,\" Social Networks and Health Workshop, 2019. Available at:  https://sites.duke.edu/dnac/15-network-visualization-and-communities/\n",
    "\n",
    "Jayawickrama, Thamindu Dilshan., \"Community Detection Algorithms,\" Towards Data Science, 2021. Available at: https://towardsdatascience.com/community-detection-algorithms-9bd8951e7dae\n",
    "\n",
    "Noesis, \"Community Detection Algorithms,\" Network-Oriented Exploration, Simulation, and Induction System. Available at: https://noesis.ikor.org/wiki/algorithms/community-detection\n",
    "\n",
    "McNulty, Keith, \"Components, Communities and Cliques\", Handbook of Graphs and Networks in People Analytics, 2022. Available at: https://ona-book.org/community.html\n",
    "\n",
    "Das, Angel, \"Social Network Analysis and Spectral Clustering in Graphs and Networks\", Towards Data Science, 2022. Available at: https://towardsdatascience.com/social-network-analysis-and-spectral-clustering-in-graphs-and-networks-40c8d878e946\n",
    "\n",
    "Doshi, Neerja, \"Spectral clustering: The intuition and math behind how it works!\", Towards Data Science, 2019. Available at: https://towardsdatascience.com/spectral-clustering-82d3cff3d3b7\n",
    "\n",
    "Maklin, Cory, \"Spectral Clustering Algorithm Implemented From Scratch\", Towards Data Science, 2019. Available at: https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045\n",
    "\n",
    "Tomar, Anmol, \"Stop Using Elbow Method in K-means Clustering, Instead, Use this!\", Towards Data Science, 2022. Available at:https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905bc8b-eeba-4e13-870c-879d02d5335e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
